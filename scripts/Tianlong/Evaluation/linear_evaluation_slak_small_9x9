#!/bin/bash
#SBATCH --job-name=knn_vit_tiny_8x8_128_300epochs
#SBATCH -p gpu
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=1
#SBATCH --gpus=1
#SBATCH -t 0-0:59:59
#SBATCH --cpus-per-task=18
#SBATCH -o knn_vit_tiny_8x8_128_300epochs.out

source /home/sliu/miniconda3/etc/profile.d/conda.sh
source activate slak


CUDA_VISIBLE_DEVICES=4,5,6,7 nohup python -m torch.distributed.launch --nproc_per_node=4 --master_port=56484  eval_linear.py --data_path /datadrive_c/imagenet/ \
--arch SLaK_small --kernel_size 9 9 9 9 100 \
--pretrained_weights /datadrive_c/ssl/slak_9_bn_300_small/checkpoint.pth \
--output_dir /datadrive_c/ssl/slak_9_bn_300_small/checkpoint.pth --checkpoint_key teacher  > ./linear_slak_small_9_bn_300.tex 2>&1 &


source deactivate