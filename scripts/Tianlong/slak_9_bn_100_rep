#!/bin/bash

nohup  CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  python -m torch.distributed.launch --nproc_per_node=8 main_dino.py --bn True \
--arch SLaK_tiny --kernel_size 9 9 9 9 3 --epochs 100 --batch_size_per_gpu 64 \
--data_path /datadrive_c/imagenet/train --output_dir /datadrive_c/ssl/slak_9_bn_300_tiny_rep3/ > slak_9_bn_100_rep.txt 2>&1 &
