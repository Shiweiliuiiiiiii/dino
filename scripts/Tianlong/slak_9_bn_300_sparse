#!/bin/bash
#SBATCH --job-name=dino_slak_9_bn_300e_sparse
#SBATCH -p gpu
#SBATCH -N 1
#SBATCH --ntasks-per-node=4
#SBATCH --gpus-per-node=4
#SBATCH --gpus=4
#SBATCH -t 3-11:59:59
#SBATCH --exclusive
#SBATCH --cpus-per-task=18
#SBATCH -o dino_slak_9_bn_300e_sparse.out

source /home/sliu/miniconda3/etc/profile.d/conda.sh
source activate LoRA

nohup python -m torch.distributed.launch --nproc_per_node=8 main_dino_sparse.py --bn True \
--sparse --width_factor 1.3 --fix --sparsity 0.4 --sparse_init snip --growth random  \
--arch SLaK_tiny --kernel_size 9 9 9 9 100 --epochs 100 --batch_size_per_gpu 64 \
--data_path /datadrive_c/imagenet/train  \
--output_dir /datadrive_c/ssl/slak_9_bn_300_sparse/ > slak_tiny_sparse_300.tex 2>&1 &


source deactivate