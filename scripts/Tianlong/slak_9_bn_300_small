#!/bin/bash

nohup CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  python -m torch.distributed.launch --nproc_per_node=8 main_dino.py --bn True --teacher_temp 0.07 \
--warmup_teacher_temp_epochs 30 --norm_last_layer false  \
--arch SLaK_small --kernel_size 9 9 9 9 100 --epochs 300 --batch_size_per_gpu 64 \
--data_path /datadrive_c/imagenet/train  \
--output_dir /datadrive_c/ssl/slak_9_bn_300_small/  > slak_9_bn_tiny_small.txt 2>&1 &
